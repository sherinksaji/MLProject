{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "\n",
    "def load_dev_in_data(file_path):\n",
    "    \"\"\"Specific function to load dev.in data, which only contains words.\"\"\"\n",
    "    \n",
    "    # Open the file for reading using utf-8 encoding\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Read the content of the file, remove leading/trailing whitespace,\n",
    "        # and split the content into separate sentences based on empty lines.\n",
    "        data = file.read().strip().split('\\n\\n')\n",
    "    \n",
    "    # Process each sentence and split it into words while removing any empty lines\n",
    "    # to create a list of sentences, where each sentence is represented as a list of words.\n",
    "    return [[word for word in sentence.split('\\n') if word.strip()] for sentence in data]\n",
    "\n",
    "\n",
    "def load_data_modified_v7(file_path):\n",
    "    \n",
    "    # Open the file for reading using utf-8 encoding\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Read the content of the file, remove leading/trailing whitespace,\n",
    "        # and split the content into separate sentences based on empty lines.\n",
    "        data = file.read().strip().split('\\n\\n')\n",
    "    \n",
    "    # Initialize an empty list to store the processed data\n",
    "    processed_data = []\n",
    "    \n",
    "    # Iterate over each sentence in the data\n",
    "    for sentence in data:\n",
    "        \n",
    "        # Initialize an empty list to store the processed words and tags for the current sentence\n",
    "        processed_sentence = []\n",
    "        \n",
    "        # Iterate over each line in the sentence\n",
    "        for line in sentence.split('\\n'):\n",
    "            \n",
    "            # Check if line is not empty\n",
    "            if line.strip():  \n",
    "                \n",
    "                # Use regular expression to match a word and a tag at the end of the line\n",
    "                match = re.search(r'^(.*)\\s(\\S+)$', line)\n",
    "                \n",
    "                # If match is true\n",
    "                if match:\n",
    "                    # Extract the word and tag using groups() method of the match object\n",
    "                    word, tag = match.groups()\n",
    "                    \n",
    "                    # Combine the word and tag with a space and append to the processed_sentence list\n",
    "                    processed_sentence.append(f\"{word} {tag}\")\n",
    "        \n",
    "        # Add the processed sentence to the data only if it's not empty\n",
    "        if processed_sentence:\n",
    "            processed_data.append(processed_sentence)\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "\n",
    "def compute_probabilities_v2(data, state_list):\n",
    "    \n",
    "    # Create a dictionary to store the count of starting transitions, count of transitions from one state to other\n",
    "    # coutn of emissions for each state and count of occurences for each state and count of occurences for each state\n",
    "    start_transition_count = {state: 0 for state in state_list}\n",
    "    transition_count = {state: {state2: 0 for state2 in state_list} for state in state_list}\n",
    "    emission_count = {state: {} for state in state_list}\n",
    "    state_count = {state: 0 for state in state_list}\n",
    "    \n",
    "    # Iterate over each sentence in the provided data\n",
    "    for sentence in data:\n",
    "        # Initialize a variable to keep track of the previous state in the sentence\n",
    "        prev_state = None\n",
    "        # Iterate over each line (word and tag pair) in the sentence\n",
    "        for line in sentence:\n",
    "            # Use regular expression to extract word and state from the line\n",
    "            match = re.search(r'^(.*)\\s(\\S+)$', line.strip())\n",
    "            # Check if the regular expression match was successful\n",
    "            if match:\n",
    "                # Extract word and state using groups() method of the match object\n",
    "                word, state = match.groups()\n",
    "                # Check if this is the beginning of a sentence (no previous state)\n",
    "                if prev_state is None:\n",
    "                    # Increment the count of starting transitions for the current state\n",
    "                    start_transition_count[state] += 1\n",
    "                else:\n",
    "                    # Increment the count of transitions from the previous state to the current state\n",
    "                    transition_count[prev_state][state] += 1\n",
    "                    # Increment the emission count for the previous state and the current word\n",
    "                    # If the word has not been encountered before for the current state, initialize the count to 0\n",
    "                    emission_count[prev_state][word] = emission_count[prev_state].get(word, 0) + 1\n",
    "                # Increment the count of occurrences for the current state\n",
    "                state_count[state] += 1\n",
    "                # Update the previous state for the next iteration\n",
    "                prev_state = state\n",
    "        # After processing all lines in the sentence, check if there is a previous state\n",
    "        if prev_state:\n",
    "            # Increment the emission count for the last state and word\n",
    "            # If the word has not been encountered before for the last state, initialize the count to 0\n",
    "            emission_count[prev_state][word] = emission_count[prev_state].get(word, 0) + 1\n",
    "            \n",
    "    # Calculate the total number of sentences in the data\n",
    "    total_sentences = len(data)\n",
    "    # Calculate the probabilities for start transitions based on the counts\n",
    "    start_transition_prob = {state: count / total_sentences for state, count in start_transition_count.items()}\n",
    "    # Calculate the probabilities for transitions based on the counts\n",
    "    transition_prob = {state: {state2: count2 / state_count[state] for state2, count2 in count.items()} for state, count in transition_count.items()}\n",
    "    # Calculate the probabilities for emissions (words) based on the counts\n",
    "    emission_prob = {state: {word: count / state_count[state] for word, count in state_emission_count.items()} for state, state_emission_count in emission_count.items()}\n",
    "    # Return the calculated probabilities for start transitions, transitions, and emissions\n",
    "    return start_transition_prob, transition_prob, emission_prob\n",
    "\n",
    "def extract_entities_from_tags(tags):\n",
    "    \n",
    "    # Initialize a list to store extracted entities\n",
    "    entities = []\n",
    "    \n",
    "    # Initialize a list to store the current entity being processed\n",
    "    entity = []\n",
    "    \n",
    "    # Iterate over each tag in the provided list of tags\n",
    "    for tag in tags:\n",
    "        # Checking for \"B-\" tag\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if entity:\n",
    "                # If there was an ongoing entity, append it to the entities list\n",
    "                entities.append(tuple(entity))\n",
    "                # Clear the entity list for the new entity\n",
    "                entity = []\n",
    "            \n",
    "            # Add the tag to the current entity\n",
    "            entity.append(tag)\n",
    "            \n",
    "        # Checking entity starts with I- tag\n",
    "        elif tag.startswith(\"I-\"):\n",
    "            # Add the tag to the current entity\n",
    "            entity.append(tag)\n",
    "              \n",
    "        else:\n",
    "            if entity:\n",
    "                # If there was an ongoing entity, append it to the entities list\n",
    "                entities.append(tuple(entity))\n",
    "                # Clear the entity list\n",
    "                entity = []\n",
    "    \n",
    "    # Check if there is any remaining entity after processing all tags\n",
    "    if entity:\n",
    "        # Append the last entity to the entities list\n",
    "        entities.append(tuple(entity))\n",
    "    \n",
    "    # Convert the list of entities into a set to remove duplicates\n",
    "    return set(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>x</th>\n",
       "      <th>!</th>\n",
       "      <th>\"</th>\n",
       "      <th>#UNK#</th>\n",
       "      <th>%</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>,</th>\n",
       "      <th>-</th>\n",
       "      <th>.</th>\n",
       "      <th>..</th>\n",
       "      <th>...</th>\n",
       "      <th>Éramos</th>\n",
       "      <th>éramos</th>\n",
       "      <th>último</th>\n",
       "      <th>única</th>\n",
       "      <th>único</th>\n",
       "      <th>’</th>\n",
       "      <th>“</th>\n",
       "      <th>”</th>\n",
       "      <th>…</th>\n",
       "      <th>€</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B-negative</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-neutral</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-positive</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.006211</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-negative</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-neutral</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-positive</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.02000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0.003501</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.005001</td>\n",
       "      <td>0.005001</td>\n",
       "      <td>0.057764</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.060015</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.00125</td>\n",
       "      <td>0.00125</td>\n",
       "      <td>0.002501</td>\n",
       "      <td>0.0015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 950 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "x                  !        \"     #UNK#        %         (         )  \\\n",
       "y                                                                      \n",
       "B-negative  0.000000  0.00000  0.016129  0.00000  0.000000  0.000000   \n",
       "B-neutral   0.000000  0.00000  0.111111  0.00000  0.000000  0.000000   \n",
       "B-positive  0.000000  0.00000  0.006211  0.00000  0.000000  0.000000   \n",
       "I-negative  0.000000  0.00000  0.027027  0.00000  0.000000  0.000000   \n",
       "I-neutral   0.000000  0.00000  0.100000  0.00000  0.000000  0.000000   \n",
       "I-positive  0.000000  0.02000  0.020000  0.00000  0.000000  0.000000   \n",
       "O           0.003501  0.00075  0.000250  0.00025  0.005001  0.005001   \n",
       "\n",
       "x                  ,       -         .       ..  ...   Éramos   éramos  \\\n",
       "y                                                ...                     \n",
       "B-negative  0.000000  0.0000  0.000000  0.00000  ...  0.00000  0.00000   \n",
       "B-neutral   0.000000  0.0000  0.000000  0.00000  ...  0.00000  0.00000   \n",
       "B-positive  0.000000  0.0000  0.000000  0.00000  ...  0.00000  0.00000   \n",
       "I-negative  0.000000  0.0000  0.000000  0.00000  ...  0.00000  0.00000   \n",
       "I-neutral   0.000000  0.0000  0.000000  0.00000  ...  0.00000  0.00000   \n",
       "I-positive  0.000000  0.0000  0.020000  0.00000  ...  0.00000  0.00000   \n",
       "O           0.057764  0.0015  0.060015  0.00025  ...  0.00075  0.00025   \n",
       "\n",
       "x           último    única   único        ’        “        ”         …  \\\n",
       "y                                                                          \n",
       "B-negative  0.0000  0.00000  0.0000  0.00000  0.00000  0.00000  0.000000   \n",
       "B-neutral   0.0000  0.00000  0.0000  0.00000  0.00000  0.00000  0.000000   \n",
       "B-positive  0.0000  0.00000  0.0000  0.00000  0.00000  0.00000  0.000000   \n",
       "I-negative  0.0000  0.00000  0.0000  0.00000  0.00000  0.00000  0.000000   \n",
       "I-neutral   0.0000  0.00000  0.0000  0.00000  0.00000  0.00000  0.000000   \n",
       "I-positive  0.0000  0.00000  0.0000  0.00000  0.00000  0.00000  0.000000   \n",
       "O           0.0005  0.00025  0.0005  0.00025  0.00125  0.00125  0.002501   \n",
       "\n",
       "x                €  \n",
       "y                   \n",
       "B-negative  0.0000  \n",
       "B-neutral   0.0000  \n",
       "B-positive  0.0000  \n",
       "I-negative  0.0000  \n",
       "I-neutral   0.0000  \n",
       "I-positive  0.0000  \n",
       "O           0.0015  \n",
       "\n",
       "[7 rows x 950 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>x</th>\n",
       "      <th>!</th>\n",
       "      <th>\"</th>\n",
       "      <th>#UNK#</th>\n",
       "      <th>%</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>,</th>\n",
       "      <th>-</th>\n",
       "      <th>.</th>\n",
       "      <th>..</th>\n",
       "      <th>...</th>\n",
       "      <th>эту</th>\n",
       "      <th>юбилей</th>\n",
       "      <th>юбилея</th>\n",
       "      <th>я</th>\n",
       "      <th>явно</th>\n",
       "      <th>язык</th>\n",
       "      <th>яйцом</th>\n",
       "      <th>января</th>\n",
       "      <th>–</th>\n",
       "      <th>…</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B-negative</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-neutral</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-positive</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-negative</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-neutral</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-positive</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0.026163</td>\n",
       "      <td>0.005630</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.006624</td>\n",
       "      <td>0.015234</td>\n",
       "      <td>0.090744</td>\n",
       "      <td>0.009604</td>\n",
       "      <td>0.052823</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.002815</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 1598 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "x                  !         \"     #UNK#         %         (         )  \\\n",
       "y                                                                        \n",
       "B-negative  0.000000  0.042254  0.014085  0.000000  0.000000  0.000000   \n",
       "B-neutral   0.000000  0.000000  0.034483  0.000000  0.000000  0.000000   \n",
       "B-positive  0.000000  0.000000  0.003425  0.000000  0.000000  0.000000   \n",
       "I-negative  0.000000  0.130435  0.043478  0.000000  0.000000  0.000000   \n",
       "I-neutral   0.000000  0.000000  0.100000  0.000000  0.000000  0.000000   \n",
       "I-positive  0.000000  0.015152  0.007576  0.000000  0.000000  0.000000   \n",
       "O           0.026163  0.005630  0.000166  0.000331  0.006624  0.015234   \n",
       "\n",
       "x                  ,         -         .        ..  ...       эту    юбилей  \\\n",
       "y                                                   ...                       \n",
       "B-negative  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "B-neutral   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "B-positive  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "I-negative  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "I-neutral   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "I-positive  0.007576  0.007576  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "O           0.090744  0.009604  0.052823  0.001159  ...  0.000166  0.000166   \n",
       "\n",
       "x             юбилея         я      явно      язык     яйцом    января  \\\n",
       "y                                                                        \n",
       "B-negative  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "B-neutral   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "B-positive  0.000000  0.000000  0.000000  0.003425  0.000000  0.000000   \n",
       "I-negative  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "I-neutral   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "I-positive  0.000000  0.000000  0.000000  0.000000  0.007576  0.000000   \n",
       "O           0.000166  0.002815  0.000166  0.000000  0.000000  0.000166   \n",
       "\n",
       "x                  –         …  \n",
       "y                               \n",
       "B-negative  0.000000  0.000000  \n",
       "B-neutral   0.000000  0.000000  \n",
       "B-positive  0.000000  0.000000  \n",
       "I-negative  0.000000  0.000000  \n",
       "I-neutral   0.000000  0.000000  \n",
       "I-positive  0.000000  0.000000  \n",
       "O           0.000331  0.000828  \n",
       "\n",
       "[7 rows x 1598 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_dataset_final_v5(dataset_type):\n",
    "    \n",
    "    # Adjusting the paths dynamically based on dataset type\n",
    "    train_path = f\"Data/{dataset_type}/train\"\n",
    "    dev_in_path = f\"Data/{dataset_type}/dev.in\"\n",
    "    dev_out_path = f\"Data/{dataset_type}/dev.out\"\n",
    "    \n",
    "    # Load training data using the modified version 7 of the data loading function\n",
    "    train_data = load_data_modified_v7(train_path)\n",
    "    \n",
    "    # Load dev.in data using the specialized data loading function\n",
    "    dev_in_data = load_dev_in_data(dev_in_path)\n",
    "    \n",
    "    # Load dev.out data (actual tags) from the corresponding file\n",
    "    with open(dev_out_path, 'r', encoding='utf-8') as f:\n",
    "        # Read the entire content of the file, remove leading/trailing whitespace,\n",
    "        # and split the content into separate sentences based on double newline characters.\n",
    "        dev_tags_actual = [sentence.split() for sentence in f.read().strip().split('\\n\\n')]\n",
    "    \n",
    "    # Initialize state and observations dictionary\n",
    "    states = {}\n",
    "    observations = {}\n",
    "\n",
    "    # Iterate over each sentence in the training data\n",
    "    for sentence in train_data:\n",
    "        \n",
    "        # Iterate over each line (word and tag pair) in the sentence\n",
    "        for line in sentence:\n",
    "            \n",
    "            # Use regular expression to extract word and tag from the line\n",
    "            match = re.search(r'^(.*)\\s(\\S+)$', line.strip())\n",
    "            \n",
    "            if match:\n",
    "                # Extract word and tag using groups() method of the match object\n",
    "                word, tag = match.groups()\n",
    "                \n",
    "                # Increment the count of occurrences for the current tag in the 'states' dictionary\n",
    "                # If the tag has not been encountered before, initialize the count to 0\n",
    "                states[tag] = states.get(tag, 0) + 1\n",
    "                \n",
    "                # Check if the tag is not already in the 'observations' dictionary\n",
    "                if tag not in observations:\n",
    "                    # Initialize a sub-dictionary for the tag\n",
    "                    observations[tag] = {}\n",
    "                    \n",
    "                # Increment the count of occurrences for the current word under the current tag\n",
    "                # If the word has not been encountered before for the current tag, initialize the count to 0\n",
    "                observations[tag][word] = observations[tag].get(word, 0) + 1\n",
    "\n",
    "    # Create a list of states by extracting keys from the 'states' dictionary\n",
    "    state_list = list(states.keys())\n",
    "    \n",
    "    # Compute probabilities for the Hidden Markov Model (HMM) using the training data and state list\n",
    "    # The resulting probabilities include start transition, transition, and emission probabilities\n",
    "    start_transition_prob, transition_prob, emission_prob = compute_probabilities_v2(train_data, state_list)\n",
    "\n",
    "    return (start_transition_prob, transition_prob)\n",
    "\n",
    "\n",
    "# Process the \"ES\" dataset and retrieve start probabilities and transition probabilities\n",
    "es_tuple = process_dataset_final_v5(\"ES\")\n",
    "sr_start_probabilities_es = es_tuple[0]\n",
    "df_transition_es = es_tuple[1]\n",
    "\n",
    "# Process the \"RU\" dataset and retrieve start probabilities and transition probabilities\n",
    "ru_tuple = process_dataset_final_v5(\"RU\")\n",
    "sr_start_probabilities_ru = ru_tuple[0]\n",
    "df_transition_ru = ru_tuple[1]\n",
    "\n",
    "\n",
    "# Convert start probabilities for the \"ES\" dataset into a Pandas Series\n",
    "sr_start_probabilities_es = pd.Series(sr_start_probabilities_es)\n",
    "\n",
    "# Convert transition probabilities for the \"ES\" dataset into a transposed Pandas DataFrame\n",
    "df_transition_es = pd.DataFrame(df_transition_es).T\n",
    "\n",
    "# Read emission probabilities for the \"ES\" dataset from a CSV file\n",
    "df_emission_es = pd.read_csv('Data/ES/csv_dev_in_es_test_e_x_y.csv')\n",
    "\n",
    "# Drop duplicates based on columns 'x' and 'y', keeping the last occurrence\n",
    "df_emission_es = df_emission_es.drop_duplicates(subset=['x','y'], keep='last')\n",
    "\n",
    "# Pivot the emission DataFrame to create a matrix with 'y' as index and 'x' as columns\n",
    "# Fill any missing values with 0\n",
    "df_emission_es = df_emission_es.pivot(index='y', columns='x', values='e(x|y)').fillna(0)\n",
    "\n",
    "# Display the emission DataFrame for the \"ES\" dataset\n",
    "display(df_emission_es)\n",
    "\n",
    "# Convert start probabilities for the \"RU\" dataset into a Pandas Series\n",
    "sr_start_probabilities_ru = pd.Series(sr_start_probabilities_ru)\n",
    "\n",
    "# Convert transition probabilities for the \"RU\" dataset into a transposed Pandas DataFrame\n",
    "df_transition_ru = pd.DataFrame(df_transition_ru).T\n",
    "\n",
    "# Read emission probabilities for the \"RU\" dataset from a CSV file\n",
    "df_emission_ru = pd.read_csv('Data/RU/csv_dev_in_ru_test_e_x_y.csv')\n",
    "\n",
    "# Drop duplicates based on columns 'x' and 'y', keeping the last occurrence\n",
    "df_emission_ru = df_emission_ru.drop_duplicates(subset=['x','y'], keep='last')\n",
    "\n",
    "# Pivot the emission DataFrame to create a matrix with 'y' as index and 'x' as columns\n",
    "# Fill any missing values with 0\n",
    "df_emission_ru = df_emission_ru.pivot(index='y', columns='x', values='e(x|y)').fillna(0)\n",
    "\n",
    "# Display the emission DataFrame for the \"RU\" dataset\n",
    "display(df_emission_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(observation, transition, emission, start_probabilities):\n",
    "    \n",
    "    # Extract states from the transition DataFrame\n",
    "    states = transition.index.tolist()\n",
    "    state_count = transition.shape[0]\n",
    "    step_count = len(observation)\n",
    "    \n",
    "    # Create a DataFrame to memoize the incoming edges for each node in the Viterbi trellis diagram\n",
    "    df_incoming = np.zeros((state_count , step_count), dtype=int)\n",
    "    df_incoming = pd.DataFrame(df_incoming, index = states)\n",
    "    \n",
    "    # Create a series to memoize the overall probabilities of arriving at each state at a particular step\n",
    "    # Calculate the step probabilities using emission and start probabilities\n",
    "    try:\n",
    "        step_probabilities = emission[observation[0]] * start_probabilities\n",
    "    except:\n",
    "        step_probabilities = emission[\"#UNK#\"] * start_probabilities\n",
    "    step_probabilities_temp = pd.Series({state: 0 for state in states})\n",
    "    \n",
    "    # Iterate over each step in the observation sequence (excluding the first step)\n",
    "    for step in range(1, step_count):\n",
    "        # Iterate over each current state\n",
    "        for current_state in states:\n",
    "            # Create a Series to track probabilities of choosing the maximum path from previous states to the current state\n",
    "            choosing_max = pd.Series({state: 0 for state in states})\n",
    "            # Iterate over each previous state\n",
    "            for previous_state in states:\n",
    "                try:\n",
    "                    # Calculate the probability of choosing the maximum path using Viterbi algorithm formula\n",
    "                    # Multiply step probability, transition probability, and emission probability\n",
    "                    choosing_max.loc[previous_state] = step_probabilities.loc[previous_state] * transition.loc[previous_state, current_state] * emission.loc[current_state, observation[step]]\n",
    "                except:\n",
    "                    # If emission probability for the current observation is not available, use emission for \"#UNK#\"\n",
    "                    choosing_max.loc[previous_state] = step_probabilities.loc[previous_state] * transition.loc[previous_state, current_state] * emission.loc[current_state, \"#UNK#\"]\n",
    "            # Find the state with the maximum calculated probability\n",
    "            max_state = choosing_max.idxmax()\n",
    "            # Update the step_probabilities_temp Series with the maximum probability for the current state\n",
    "            step_probabilities_temp.loc[current_state] = choosing_max.loc[max_state]\n",
    "            # Memoize the incoming state that leads to the maximum probability in the df_incoming DataFrame\n",
    "            df_incoming.loc[current_state, step] = max_state\n",
    "        # Update the step_probabilities Series for the next step using the updated probabilities from step_probabilities_temp\n",
    "        step_probabilities = step_probabilities_temp.copy()\n",
    "    \n",
    "    # get sequences in descending order of likelihood\n",
    "    descending_final_probability_states = step_probabilities.sort_values(ascending=False).index.to_numpy()    \n",
    "    # Create a DataFrame to store sequences of states\n",
    "    df_sequences = np.zeros((state_count , step_count), dtype=int)\n",
    "    df_sequences = pd.DataFrame(df_sequences)\n",
    "    # Initialize the last step of each sequence with the most likely states\n",
    "    for sequence_rank in range(state_count):\n",
    "        df_sequences.loc[sequence_rank, step_count-1] = descending_final_probability_states[sequence_rank]\n",
    "    # Iterate through steps and fill in the rest of the sequences using memoized incoming edges\n",
    "    for sequence_rank in range(state_count):\n",
    "        for step in range(step_count-2, -1, -1):\n",
    "            # Get the previous state of the current step from the memoized incoming edges\n",
    "            previous_state = df_sequences.loc[sequence_rank, step+1]\n",
    "            # Assign the previous state to the current step in the sequence\n",
    "            df_sequences.loc[sequence_rank, step] = df_incoming.loc[previous_state, step+1]\n",
    "    # Return the sequence of states with the highest likelihood (most likely sequence)\n",
    "    return df_sequences.loc[0].values\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    # Open the file for reading using UTF-8 encoding\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Read the entire content of the file, remove leading/trailing whitespace,\n",
    "        # and split the content into separate sentences based on double newline characters.\n",
    "        data = file.read().strip().split('\\n\\n')\n",
    "    \n",
    "    # Split each sentence into lines using newline characters\n",
    "    # The result is a list of lists, where each inner list represents a sentence\n",
    "    # and contains lines of text from the file.\n",
    "    return [sentence.split('\\n') for sentence in data]\n",
    "\n",
    "# Load observations (words) for the \"ES\" dataset from the 'dev.in' file\n",
    "es_observations = load_data('Data/ES/dev.in')\n",
    "\n",
    "# Apply the Viterbi algorithm to predict the most likely sequence of tags (hidden states)\n",
    "# for each sentence in the \"ES\" dataset using the precomputed transition and emission probabilities,\n",
    "# as well as the start probabilities for the Hidden Markov Model (HMM)\n",
    "predicted_tags_viterbi_ES1 = [viterbi([word.split()[0] for word in sentence], df_transition_es, df_emission_es, sr_start_probabilities_es) for sentence in es_observations]\n",
    "\n",
    "# Create an empty list to store the combined predictions for the \"ES\" dataset\n",
    "predicted_ES1 = []\n",
    "\n",
    "# Iterate through each sentence in the predicted Viterbi tags for the \"ES\" dataset\n",
    "for s in range(len(predicted_tags_viterbi_ES1)):\n",
    "    \n",
    "    # Iterate through each word's index in the current sentence\n",
    "    for i in range(len(predicted_tags_viterbi_ES1[s])):\n",
    "        \n",
    "        # Combine the observed word with the predicted tag from Viterbi and append it to predicted_ES1 list\n",
    "        predicted_ES1.append(es_observations[s][i] + \" \"+ predicted_tags_viterbi_ES1[s][i])\n",
    "    \n",
    "    # Append a newline character to separate predictions for different sentences\n",
    "    predicted_ES1.append('\\n')\n",
    "\n",
    "# Open the 'dev.p2.out' file for writing using UTF-8 encoding\n",
    "with open('Data/ES/dev.p2.out', 'w', encoding='utf-8') as file:\n",
    "    # Write each modified line back to the output file\n",
    "    for line in predicted_ES1:\n",
    "        if line != \"\\n\":\n",
    "            # Write the line followed by a newline character\n",
    "            file.write(line + '\\n')\n",
    "        else:\n",
    "            # Write the newline character without an additional newline\n",
    "            file.write(line)\n",
    "            \n",
    "            es_observations = load_data('Data/ES/dev.in')\n",
    "            \n",
    "# Load observations (words) for the \"ES\" dataset from the 'dev.in' file\n",
    "ru_observations = load_data('Data/RU/dev.in')\n",
    "\n",
    "# Apply the Viterbi algorithm to predict the most likely sequence of tags (hidden states)\n",
    "# for each sentence in the \"RU\" dataset using the precomputed transition and emission probabilities,\n",
    "# as well as the start probabilities for the Hidden Markov Model (HMM)\n",
    "predicted_tags_viterbi_RU1 = [viterbi([word.split()[0] for word in sentence], df_transition_ru, df_emission_ru, sr_start_probabilities_ru) for sentence in ru_observations]\n",
    "\n",
    "# Create an empty list to store the combined predictions for the \"RU\" dataset\n",
    "predicted_RU1 = []\n",
    "\n",
    "# Iterate through each sentence in the predicted Viterbi tags for the \"RU\" dataset\n",
    "for s in range(len(predicted_tags_viterbi_RU1)):\n",
    "    \n",
    "    # Iterate through each word's index in the current sentence\n",
    "    for i in range(len(predicted_tags_viterbi_RU1[s])):\n",
    "        \n",
    "        # Combine the observed word with the predicted tag from Viterbi and append it to predicted_RU list\n",
    "        predicted_RU1.append(ru_observations[s][i] + \" \"+ predicted_tags_viterbi_RU1[s][i])\n",
    "    \n",
    "    # Append a newline character to separate predictions for different sentences\n",
    "    predicted_RU1.append('\\n')\n",
    "\n",
    "# Open the 'dev.p2.out' file for writing using UTF-8 encoding\n",
    "with open('Data/RU/dev.p2.out', 'w', encoding='utf-8') as file:\n",
    "    \n",
    "    # Write each modified line back to the output file\n",
    "    for line in predicted_RU1:\n",
    "        if line != \"\\n\":\n",
    "            # Write the line followed by a newline character\n",
    "            file.write(line + '\\n')\n",
    "        \n",
    "        else:\n",
    "            # Write the newline character without an additional newline\n",
    "            file.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
