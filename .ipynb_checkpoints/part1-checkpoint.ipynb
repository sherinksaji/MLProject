{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a function that estimates the emission parameters from the training set using MLE (maximum likelihoodestimation):\n",
    "Which training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file_to_df is a function to convert some data file into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_df (file_path):\n",
    "\n",
    "\n",
    "  with open(file_path, 'r',encoding='utf-8') as file:\n",
    "      lines = file.readlines()\n",
    "      data = [line.strip().split(' ', maxsplit=2)[:2] for line in lines]\n",
    "\n",
    "  # Create the dataframe\n",
    "  df = pd.DataFrame(data, columns=['x', 'y'])\n",
    "  #drop rows where value for y=None\n",
    "  df = df.dropna(subset=['y'])\n",
    "\n",
    "  df['x'] = df['x'].astype(str)\n",
    "  df['y'] = df['y'].astype(str)\n",
    "  # Display the dataframe\n",
    "  #print(\"Here is dataframe created for the file path: \",file_path,\"\\n\")\n",
    "  #print(df.to_string)\n",
    "  return df\n",
    "\n",
    "file_path_train_es='Data/ES/train'\n",
    "df_train_es=file_to_df(file_path_train_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_y(df,y_value):\n",
    "    unique_counts = df['y'].value_counts().to_dict()\n",
    "    print(\"unique counts:\",unique_counts)\n",
    "    return unique_counts[y_value]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_star_with_smallest_count(df):\n",
    "     unique_counts = df['y_star'].value_counts().to_dict()\n",
    "     min_count = min(unique_counts.values())\n",
    "     y_with_min_count = [key for key, value in unique_counts.items() if value == min_count]         \n",
    "     return y_with_min_count[0]         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_filtered_for_y_value(df,y_value):\n",
    "  return df[df['y'] == y_value]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_x_count_y_to_x(df):\n",
    "    df_x_count_y_to_x = df['x'].value_counts().reset_index()\n",
    "\n",
    "    df_x_count_y_to_x.columns = ['x', 'count_y_to_x']\n",
    "\n",
    "    return df_x_count_y_to_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O' 'B-positive' 'B-negative' 'B-neutral' 'I-neutral' 'I-positive'\n",
      " 'I-negative']\n"
     ]
    }
   ],
   "source": [
    "def create_ls_of_all_y_values(df):\n",
    "    unique_values = df['y'].unique()\n",
    "    return (unique_values)\n",
    "print(create_ls_of_all_y_values(df_train_es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique counts: {'O': 29035, 'B-positive': 1160, 'B-negative': 381, 'I-positive': 314, 'I-negative': 171, 'B-neutral': 72, 'I-neutral': 43}\n",
      "               x  count_y_to_x    e(x|y)           y\n",
      "0         comida           169  0.145690  B-positive\n",
      "1       servicio           122  0.105172  B-positive\n",
      "2    restaurante            46  0.039655  B-positive\n",
      "3          trato            44  0.037931  B-positive\n",
      "4       ambiente            33  0.028448  B-positive\n",
      "..           ...           ...       ...         ...\n",
      "280       dorada             1  0.000862  B-positive\n",
      "281      detalle             1  0.000862  B-positive\n",
      "282   cantidades             1  0.000862  B-positive\n",
      "283       Atteca             1  0.000862  B-positive\n",
      "284        menus             1  0.000862  B-positive\n",
      "\n",
      "[285 rows x 4 columns]\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "def create_df_e_x_y_train(train_df,y_value):\n",
    "   \n",
    "    df_train_filtered_for_y = create_df_filtered_for_y_value(train_df,y_value)\n",
    "    df_e_x_y_train=create_df_x_count_y_to_x(df_train_filtered_for_y)\n",
    "    df_e_x_y_train['e(x|y)'] = df_e_x_y_train['count_y_to_x']/(count_y(train_df,y_value))\n",
    "    df_e_x_y_train['y']=y_value\n",
    "    return df_e_x_y_train\n",
    "\n",
    "\n",
    "df_e_x_y_train_for_I_neutral=create_df_e_x_y_train(df_train_es,\"B-positive\")\n",
    "print(df_e_x_y_train_for_I_neutral)\n",
    "print(df_e_x_y_train_for_I_neutral['e(x|y)'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique counts: {'O': 3998, 'B-positive': 160, 'B-negative': 61, 'I-positive': 49, 'I-negative': 36, 'B-neutral': 8}\n",
      "0.9937888198757765\n",
      "               x  count_y_to_x    e(x|y)\n",
      "0       servicio            24  0.149068\n",
      "1         comida            23  0.142857\n",
      "2    restaurante             8  0.049689\n",
      "3       ambiente             7  0.043478\n",
      "4         platos             6  0.037267\n",
      "..           ...           ...       ...\n",
      "67      cocinero             1  0.006211\n",
      "68        tartar             1  0.006211\n",
      "69       postres             1  0.006211\n",
      "70  localizaci√≥n             1  0.006211\n",
      "71         comer             1  0.006211\n",
      "\n",
      "[72 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def create_df_e_x_y_test(test_df,y_value,train_df):\n",
    "    k=1\n",
    "    #print(test_df.info())\n",
    "    df_test_filtered_for_y = create_df_filtered_for_y_value(test_df,y_value)\n",
    "    #print(df_test_filtered_for_y)\n",
    "    df_e_x_y_test=create_df_x_count_y_to_x(df_test_filtered_for_y)\n",
    "    #print (df_e_x_y_test)\n",
    "    \n",
    "    count_y_plus_k=count_y(test_df,y_value)+k\n",
    "\n",
    "    #make a list of all the x values in the train_df's x column\n",
    "   \n",
    "\n",
    "    \n",
    "    train_df_x_values = train_df['x'].tolist()\n",
    "    #print(train_df_x_values)\n",
    "    \n",
    "    df_e_x_y_test['x'] = df_e_x_y_test['x'].apply(lambda x: x if x in train_df_x_values else \"#UNK#\") #if word does or does not appear in training set\n",
    "    #print(df_e_x_y_test)\n",
    "    \n",
    "    df_e_x_y_test['e(x|y)'] = df_e_x_y_test.apply(lambda row: (row['count_y_to_x'] / count_y_plus_k) if row['x'] != '#UNK#' else (k / count_y_plus_k), axis=1)\n",
    "    print(df_e_x_y_test['e(x|y)'].sum())\n",
    "\n",
    "    #case 1\n",
    "    #df_e_x_y_test['e(x|y)'] = df_e_x_y_test['count_y_to_x']/count_y_plus_k\n",
    "\n",
    "    #case 2\n",
    "    #df_e_x_y_test['e(x|y)'] = k/count_y_plus_k\n",
    "\n",
    "\n",
    "    return df_e_x_y_test\n",
    "\n",
    "\n",
    "file_path_test_es='Data/ES/dev.out'\n",
    "df_test_es=file_to_df(file_path_test_es)\n",
    "df_e_x_y_test_for_I_neutral=create_df_e_x_y_test(df_test_es,\"B-positive\",df_train_es)\n",
    "print(df_e_x_y_test_for_I_neutral)\n",
    "# count_unk = 0\n",
    "# value_counts = df_e_x_y_test_for_I_neutral['x'].value_counts()\n",
    "# print(value_counts)\n",
    "# if '#UNK#' in value_counts.index:\n",
    "#     count_unk = value_counts['#UNK#']\n",
    "# print(count_unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_e_x_y_df_train_all_y_values(file_path):\n",
    "    \n",
    "  df_train=file_to_df(file_path)\n",
    "  #print(df_train_es)\n",
    "  ls_df_train=[]\n",
    "  ls_y_values=create_ls_of_all_y_values(df_train)\n",
    "  #print(len(ls_y_values))\n",
    "  for y_value in ls_y_values:\n",
    "    if y_value!=None:\n",
    "      df_e_x_y=create_df_e_x_y_train(df_train,y_value)\n",
    "      ls_df_train.append(df_e_x_y)\n",
    "  #print(len(ls_df_train))\n",
    "  \n",
    "  combined_df_train=pd.concat(ls_df_train,axis=0)\n",
    "  return (combined_df_train)\n",
    "#file_path_train_es='Data/ES/train'\n",
    "#print(create_e_x_y_df_train_all_y_values(file_path_train_es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique counts: {'O': 29035, 'B-positive': 1160, 'B-negative': 381, 'I-positive': 314, 'I-negative': 171, 'B-neutral': 72, 'I-neutral': 43}\n",
      "unique counts: {'O': 29035, 'B-positive': 1160, 'B-negative': 381, 'I-positive': 314, 'I-negative': 171, 'B-neutral': 72, 'I-neutral': 43}\n",
      "unique counts: {'O': 29035, 'B-positive': 1160, 'B-negative': 381, 'I-positive': 314, 'I-negative': 171, 'B-neutral': 72, 'I-neutral': 43}\n",
      "unique counts: {'O': 29035, 'B-positive': 1160, 'B-negative': 381, 'I-positive': 314, 'I-negative': 171, 'B-neutral': 72, 'I-neutral': 43}\n",
      "unique counts: {'O': 29035, 'B-positive': 1160, 'B-negative': 381, 'I-positive': 314, 'I-negative': 171, 'B-neutral': 72, 'I-neutral': 43}\n",
      "unique counts: {'O': 29035, 'B-positive': 1160, 'B-negative': 381, 'I-positive': 314, 'I-negative': 171, 'B-neutral': 72, 'I-neutral': 43}\n",
      "unique counts: {'O': 29035, 'B-positive': 1160, 'B-negative': 381, 'I-positive': 314, 'I-negative': 171, 'B-neutral': 72, 'I-neutral': 43}\n"
     ]
    }
   ],
   "source": [
    "def create_df_x_to_y_star(file_path):\n",
    "    e_x_y_df_train=create_e_x_y_df_train_all_y_values(file_path)\n",
    "    \n",
    "    # Group by 'x' and find the maximum 'e(x|y)' value for each group\n",
    "    df_x_to_y_star = e_x_y_df_train.groupby('x')['e(x|y)'].max().reset_index()\n",
    "\n",
    "    # Find the corresponding 'y' values for the maximum 'e(x|y)' values\n",
    "    df_x_to_y_star = pd.merge(df_x_to_y_star, e_x_y_df_train, on=['x', 'e(x|y)'])\n",
    "\n",
    "    # Rename the columns\n",
    "    df_x_to_y_star.columns = ['x', 'max_e(x|y)','count_y_to_x', 'y_star']\n",
    "\n",
    "    # Print the resulting DataFrame\n",
    "    return (df_x_to_y_star)\n",
    "    \n",
    "file_path_train_es='Data/ES/train'    \n",
    "df_x_to_y_star=create_df_x_to_y_star(file_path_train_es)\n",
    "csv_x_to_y_star = df_x_to_y_star.to_csv('train_x_to_y_star.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique counts: {'O': 29035, 'B-positive': 1160, 'B-negative': 381, 'I-positive': 314, 'I-negative': 171, 'B-neutral': 72, 'I-neutral': 43}\n",
      "unique counts: {'O': 29035, 'B-positive': 1160, 'B-negative': 381, 'I-positive': 314, 'I-negative': 171, 'B-neutral': 72, 'I-neutral': 43}\n",
      "unique counts: {'O': 29035, 'B-positive': 1160, 'B-negative': 381, 'I-positive': 314, 'I-negative': 171, 'B-neutral': 72, 'I-neutral': 43}\n",
      "unique counts: {'O': 29035, 'B-positive': 1160, 'B-negative': 381, 'I-positive': 314, 'I-negative': 171, 'B-neutral': 72, 'I-neutral': 43}\n",
      "unique counts: {'O': 29035, 'B-positive': 1160, 'B-negative': 381, 'I-positive': 314, 'I-negative': 171, 'B-neutral': 72, 'I-neutral': 43}\n",
      "unique counts: {'O': 29035, 'B-positive': 1160, 'B-negative': 381, 'I-positive': 314, 'I-negative': 171, 'B-neutral': 72, 'I-neutral': 43}\n",
      "unique counts: {'O': 29035, 'B-positive': 1160, 'B-negative': 381, 'I-positive': 314, 'I-negative': 171, 'B-neutral': 72, 'I-neutral': 43}\n"
     ]
    }
   ],
   "source": [
    "def generate_y_values(file_path_dev_in,file_path_train,file_path_dev_p1_out):\n",
    "    df_train=create_df_x_to_y_star(file_path_train)\n",
    "    x_values=df_train['x'].tolist()\n",
    "    #the y_label that has the smallest count_y will generate the biggest k / count_y_plus_k which will give the maximum e(x|y) which means that y_label is y_star\n",
    "    #so that y_label is:\n",
    "    y_label=y_star_with_smallest_count(df_train) \n",
    "    with open(file_path_dev_in, 'r',encoding='utf-8') as file:\n",
    "      lines = file.readlines()\n",
    "      for l in range(len(lines)):\n",
    "        line=lines[l].strip()\n",
    "        if line in x_values:\n",
    "          possible_y_values=df_train[df_train['x'] == line]['y_star'].tolist()\n",
    "          lines[l]=line+\" \"+possible_y_values[0]\n",
    "         \n",
    "          if (len(possible_y_values)!=1):\n",
    "            print (\"something wrong: x_vlaues in df_train not unique for some reason,for line: \",l)\n",
    "        else:\n",
    "          if (line!=\"\"):\n",
    "             line=line+\" \"+y_label\n",
    "             lines[l]=line\n",
    "          else:\n",
    "            lines[l]=\"\"\n",
    "          \n",
    "    with open(file_path_dev_p1_out, 'w',encoding='utf-8') as file:\n",
    "        \n",
    "        for line in lines:\n",
    "          file.write(line+\"\\n\")   \n",
    "         \n",
    "         \n",
    "\n",
    "   \n",
    "file_path_dev_in_es = 'Data/ES/dev.in'   \n",
    "file_path_train_es='Data/ES/train'  \n",
    "file_path_dev_p1_out_es='Data/ES/dev.p1.out' \n",
    "generate_y_values(file_path_dev_in_es,file_path_train_es,file_path_dev_p1_out_es)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4312\n",
      "4312\n",
      "0.6307977736549165\n"
     ]
    }
   ],
   "source": [
    "file_path_dev_p1_out_es='Data/ES/dev.p1.out'\n",
    "file_dev_out_es='Data/ES/dev.out'\n",
    "df_dev_p1_out_es= file_to_df (file_path_dev_p1_out_es)\n",
    "df_dev_out_es=file_to_df (file_dev_out_es)\n",
    "print(df_dev_p1_out_es.shape[0])\n",
    "print(df_dev_out_es.shape[0])\n",
    "\n",
    "df1=df_dev_p1_out_es\n",
    "df2=df_dev_out_es\n",
    "\n",
    "# Compare rows\n",
    "comparison = df1.equals(df2)\n",
    "\n",
    "# Get the count of rows that are the same\n",
    "count_same = df1[df1.eq(df2).all(axis=1)].shape[0]\n",
    "\n",
    "# Get the count of rows that are not the same\n",
    "count_not_same = df1[df1.ne(df2).any(axis=1)].shape[0]\n",
    "\n",
    "# Get the rows that are not the same\n",
    "rows_not_same = df1[~df1.eq(df2).all(axis=1)]\n",
    "\n",
    "total_number_of_correctly_predicted_entries=count_same\n",
    "total_number_of_predicted_entities=df1.shape[0]\n",
    "precision=total_number_of_correctly_predicted_entries/total_number_of_predicted_entities\n",
    "print(precision)\n",
    "\n",
    "# Display the results\n",
    "# print(\"Comparison result:\", comparison)\n",
    "# print(\"Count of rows that are the same:\", count_same)\n",
    "# print(\"Count of rows that are not the same:\", count_not_same)\n",
    "# print(\"Rows that are not the same:\\n\", rows_not_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transition_counts_es' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12220\\2567881993.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Estimate the transition probabilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mtransition_probs_es\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_transition_probabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransition_counts_es\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_train_es\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mtransition_probs_es\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transition_counts_es' is not defined"
     ]
    }
   ],
   "source": [
    "def compute_transition_probabilities(transition_counts, df):\n",
    "    \"\"\"\n",
    "    Calculate transition probabilities using the MLE formula.\n",
    "    \n",
    "    Parameters:\n",
    "    transition_counts (dict): A dictionary containing transition counts for each unique pair of y values.\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary containing transition probabilities for each unique pair of y values.\n",
    "    \"\"\"\n",
    "    \n",
    "    transition_probs = {}\n",
    "    y_counts = df['y'].value_counts().to_dict()\n",
    "\n",
    "    # Add count for 'START' tag\n",
    "    y_counts['START'] = df[df['y'] == 'O'].shape[0]\n",
    "\n",
    "    # Calculate transition probabilities using MLE formula\n",
    "    for (y_prev, y_curr), count in transition_counts.items():\n",
    "        transition_probs[(y_prev, y_curr)] = count / y_counts[y_prev]\n",
    "    \n",
    "    return transition_probs\n",
    "\n",
    "# Estimate the transition probabilities\n",
    "transition_probs_es = compute_transition_probabilities(transition_counts_es, df_train_es)\n",
    "transition_probs_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algorithm(sentence, transition_probs, emission_params, unique_tags):\n",
    "    \"\"\"\n",
    "    Implements the Viterbi algorithm to find the most probable sequence of states.\n",
    "    \n",
    "    Parameters:\n",
    "    sentence (list): List of words in the sentence to be tagged.\n",
    "    transition_probs (dict): Dictionary containing transition probabilities.\n",
    "    emission_params (dict): Dictionary containing emission parameters.\n",
    "    unique_tags (list): List of unique tags in the training data.\n",
    "    \n",
    "    Returns:\n",
    "    list: List of predicted tags for the sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of words in the sentence\n",
    "    n = len(sentence)\n",
    "    \n",
    "    # Initialize viterbi and backpointer matrices\n",
    "    viterbi = {}\n",
    "    backpointer = {}\n",
    "    \n",
    "    # Initialization step\n",
    "    for tag in unique_tags:\n",
    "        transition_key = ('START', tag)\n",
    "        emission_key = (tag, sentence[0])\n",
    "        viterbi[(tag, 0)] = transition_probs.get(transition_key, 0.00001) * emission_params.get(emission_key, 0.00001)\n",
    "        backpointer[(tag, 0)] = 'START'\n",
    "    \n",
    "    # Recursion step\n",
    "    for i in range(1, n):\n",
    "        for tag in unique_tags:\n",
    "            max_prob = 0\n",
    "            best_prev_tag = None\n",
    "            for prev_tag in unique_tags:\n",
    "                transition_key = (prev_tag, tag)\n",
    "                emission_key = (tag, sentence[i])\n",
    "                prob = viterbi[(prev_tag, i-1)] * transition_probs.get(transition_key, 0.00001) * emission_params.get(emission_key, 0.00001)\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    best_prev_tag = prev_tag\n",
    "            viterbi[(tag, i)] = max_prob\n",
    "            backpointer[(tag, i)] = best_prev_tag\n",
    "    \n",
    "    # Termination step\n",
    "    max_prob = 0\n",
    "    best_prev_tag = None\n",
    "    for tag in unique_tags:\n",
    "        transition_key = (tag, 'STOP')\n",
    "        prob = viterbi[(tag, n-1)] * transition_probs.get(transition_key, 0.00001)\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            best_prev_tag = tag\n",
    "    backpointer[('STOP', n)] = best_prev_tag\n",
    "    \n",
    "    # Backtrack to find the best path\n",
    "    best_path = []\n",
    "    current_tag = 'STOP'\n",
    "    for i in range(n, 0, -1):\n",
    "        best_path.insert(0, backpointer[(current_tag, i)])\n",
    "        current_tag = backpointer[(current_tag, i)]\n",
    "    \n",
    "    return best_path\n",
    "\n",
    "# For this implementation, I am using a subset of the development data for faster debugging and testing\n",
    "subset_dev_sentences = dev_sentences[:10]\n",
    "predicted_tags_viterbi = [viterbi_algorithm(sentence, transition_probs_es, emission_params, state_counts.keys()) for sentence in subset_dev_sentences]\n",
    "predicted_tags_viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprocessing the dev.out file to extract the actual tags\n",
    "dev_tags_actual = []\n",
    "temp_tags = []\n",
    "for _, row in df_dev_out_es.iterrows():\n",
    "    if row['x'] == '':\n",
    "        dev_tags_actual.append(temp_tags)\n",
    "        temp_tags = []\n",
    "    else:\n",
    "        temp_tags.append(row['y'])\n",
    "\n",
    "# Recalculate precision, recall, and F-scores\n",
    "true_positive, false_positive, false_negative = 0, 0, 0\n",
    "\n",
    "for true_tags, pred_tags in zip(dev_tags_actual, predicted_tags_for_dev_viterbi):\n",
    "    for t, p in zip(true_tags, pred_tags):\n",
    "        if t != 'O' and p == t:\n",
    "            true_positive += 1\n",
    "        if t != 'O' and p != t:\n",
    "            false_negative += 1\n",
    "        if p != 'O' and p != t:\n",
    "            false_positive += 1\n",
    "\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "f_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "precision, recall, f_score\n",
    "Part 2 - Version 3\n",
    "def compute_transition_probabilities(df):\n",
    "    # Add a START and STOP tag to the beginning and end of each sentence\n",
    "    df['y_shifted'] = df['y'].shift(1).fillna('START')\n",
    "    df.loc[df['y'].isna(), 'y_shifted'] = 'STOP'\n",
    "\n",
    "    # Count occurrences of tag transitions\n",
    "    transition_counts = df.groupby(['y_shifted', 'y']).size().reset_index(name='count')\n",
    "\n",
    "    # Count occurrences of each tag\n",
    "    tag_counts = df['y_shifted'].value_counts().reset_index()\n",
    "    tag_counts.columns = ['y_shifted', 'tag_count']\n",
    "\n",
    "    # Merge the two dataframes to compute the transition probabilities\n",
    "    merged_df = pd.merge(transition_counts, tag_counts, on='y_shifted')\n",
    "    merged_df['transition_probability'] = merged_df['count'] / merged_df['tag_count']\n",
    "\n",
    "    # Create a dictionary for transition probabilities\n",
    "    transition_probabilities = {}\n",
    "    for _, row in merged_df.iterrows():\n",
    "        tag_prev, tag, prob = row['y_shifted'], row['y'], row['transition_probability']\n",
    "        transition_probabilities[(tag_prev, tag)] = prob\n",
    "\n",
    "    return transition_probabilities\n",
    "\n",
    "transition_probabilities = compute_transition_probabilities(df_train)\n",
    "list(transition_probabilities.items())[:10]  # Displaying a subset of the transition probabilities for verification\n",
    "def viterbi_algorithm_modified(sentence, emission_probabilities, transition_probabilities, tags):\n",
    "    \"\"\"\n",
    "    Implement the Viterbi algorithm with modifications to handle unseen word-tag pairs and unseen tag transitions.\n",
    "    \"\"\"\n",
    "    n = len(sentence)\n",
    "    m = len(tags)\n",
    "    VERY_SMALL_PROB = 1e-10  # Small probability value for unseen transitions/emissions\n",
    "    \n",
    "    # Initialize score and backpointer matrices\n",
    "    score = [[0.0 for _ in range(m)] for _ in range(n+1)]\n",
    "    backpointer = [[None for _ in range(m)] for _ in range(n+1)]\n",
    "    \n",
    "    # Initialization step\n",
    "    for tag_idx, tag in enumerate(tags):\n",
    "        emission_key = (sentence[0], tag)\n",
    "        transition_key = ('START', tag)\n",
    "        score[0][tag_idx] = transition_probabilities.get(transition_key, VERY_SMALL_PROB) * \\\n",
    "                            emission_probabilities.get(emission_key, VERY_SMALL_PROB)\n",
    "    \n",
    "    # Recursion step\n",
    "    for i in range(1, n):\n",
    "        for tag_idx, tag in enumerate(tags):\n",
    "            max_transition_prob = 0\n",
    "            best_previous_tag_idx = None\n",
    "            \n",
    "            for prev_tag_idx, prev_tag in enumerate(tags):\n",
    "                transition_key = (prev_tag, tag)\n",
    "                prob = score[i-1][prev_tag_idx] * transition_probabilities.get(transition_key, VERY_SMALL_PROB)\n",
    "                \n",
    "                if prob > max_transition_prob:\n",
    "                    max_transition_prob = prob\n",
    "                    best_previous_tag_idx = prev_tag_idx\n",
    "            \n",
    "            emission_key = (sentence[i], tag)\n",
    "            score[i][tag_idx] = max_transition_prob * emission_probabilities.get(emission_key, VERY_SMALL_PROB)\n",
    "            backpointer[i][tag_idx] = best_previous_tag_idx\n",
    "    \n",
    "    # Termination step\n",
    "    max_transition_prob = 0\n",
    "    best_previous_tag_idx = None\n",
    "    for prev_tag_idx, prev_tag in enumerate(tags):\n",
    "        transition_key = (prev_tag, 'STOP')\n",
    "        prob = score[n-1][prev_tag_idx] * transition_probabilities.get(transition_key, VERY_SMALL_PROB)\n",
    "        \n",
    "        if prob > max_transition_prob:\n",
    "            max_transition_prob = prob\n",
    "            best_previous_tag_idx = prev_tag_idx\n",
    "    \n",
    "    score[n][0] = max_transition_prob\n",
    "    backpointer[n][0] = best_previous_tag_idx\n",
    "    \n",
    "    # Backtrack to find the best sequence of tags\n",
    "    best_sequence_of_tags = [None for _ in range(n)]\n",
    "    best_sequence_of_tags[n-1] = tags[best_previous_tag_idx]\n",
    "    \n",
    "    for i in range(n-2, -1, -1):\n",
    "        best_sequence_of_tags[i] = tags[backpointer[i+1][tags.index(best_sequence_of_tags[i+1])]]\n",
    "    \n",
    "    return best_sequence_of_tags\n",
    "\n",
    "# Testing the modified Viterbi algorithm on a small subset of the development data\n",
    "predicted_tags_viterbi_test_mod = [viterbi_algorithm_modified(sentence, emission_probabilities, transition_probabilities, tags) for sentence in test_sentences]\n",
    "\n",
    "predicted_tags_viterbi_test_mod\n",
    "# Extracting tags from the provided dev.out file\n",
    "def extract_tags_from_dev_out(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    tags = []\n",
    "    sentence_tags = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line == '':\n",
    "            tags.append(sentence_tags)\n",
    "            sentence_tags = []\n",
    " # Part 2 - Version 3       else:\n",
    "            _, tag = line.split(' ')\n",
    "            sentence_tags.append(tag)\n",
    "    if sentence_tags:\n",
    "        tags.append(sentence_tags)\n",
    "    return tags\n",
    "\n",
    "dev_tags_actual = extract_tags_from_dev_out('/mnt/data/dev.out')\n",
    "\n",
    "# Flatten the lists for comparison\n",
    "actual_flat = [tag for sublist in dev_tags_actual for tag in sublist]\n",
    "\n",
    "# Compute precision, recall, and F-scores\n",
    "TP = sum(p == a for p, a in zip(predicted_flat, actual_flat))\n",
    "FP = sum(p != a for p, a in zip(predicted_flat, actual_flat))\n",
    "FN = sum(a != p for p, a in zip(predicted_flat, actual_flat))\n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "precision, recall, f_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
