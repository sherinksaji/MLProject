{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/RU/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m file_path_train_es \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mData/RU/train\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[39m# Stores resulting dataframe in df_train_es\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m df_train_es \u001b[39m=\u001b[39m file_to_df(file_path_train_es)\n",
      "Cell \u001b[1;32mIn[32], line 13\u001b[0m, in \u001b[0;36mfile_to_df\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mRead data from a file, process it, and create a Pandas DataFrame.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mDataFrame: A DataFrame containing processed data with columns 'x' and 'y'.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# Opens the file using file_path function. \u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m     14\u001b[0m     \u001b[39m# Reads lines of file using readlines()\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     lines \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mreadlines()\n\u001b[0;32m     16\u001b[0m     \u001b[39m# So, the entire expression takes each line, removes leading/trailing whitespace,\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[39m# splits it into parts based on spaces (up to 2 splits), and then extracts the first two parts.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[39m# The result is a list of pairs of values from each line in the lines list. \u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/RU/train'"
     ]
    }
   ],
   "source": [
    "def file_to_df(file_path):\n",
    "    \"\"\"\n",
    "    Read data from a file, process it, and create a Pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the file containing the data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing processed data with columns 'x' and 'y'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Opens the file using file_path function. \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Reads lines of file using readlines()\n",
    "        lines = file.readlines()\n",
    "        # So, the entire expression takes each line, removes leading/trailing whitespace,\n",
    "        # splits it into parts based on spaces (up to 2 splits), and then extracts the first two parts.\n",
    "        # The result is a list of pairs of values from each line in the lines list. \n",
    "        data = [line.strip().split(' ', maxsplit=2)[:2] for line in lines]\n",
    "\n",
    "    # Creating dataframe from the data list with 2 columns, x and y.\n",
    "    df = pd.DataFrame(data, columns=['x', 'y'])\n",
    "\n",
    "    # Drop rows where value for y = None \n",
    "    df = df.dropna(subset=['y'])\n",
    "\n",
    "    # Convert the data type of columns 'x' and 'y' to strings\n",
    "    df['x'] = df['x'].astype(str)\n",
    "    df['y'] = df['y'].astype(str)\n",
    "\n",
    "    # Display the dataframe\n",
    "    return df\n",
    "\n",
    "# file_to_df function called with the file path\n",
    "file_path_train_es = \"Data/ES/train\"\n",
    "# Stores resulting dataframe in df_train_es\n",
    "df_train_es = file_to_df(file_path_train_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_y(df, y_value):\n",
    "    \"\"\"\n",
    "    Count the occurrences of a specific value in the 'y' column of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    y_value: The value for which the count is to be determined.\n",
    "\n",
    "    Returns:\n",
    "    int: The count of occurrences of the specified y_value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the count of each unique value in the 'y' column and convert it to a dictionary.\n",
    "    unique_counts = df['y'].value_counts().to_dict()\n",
    "\n",
    "    # Return the count of occurrences of the specified y_value\n",
    "    print(\"unique counts:\", unique_counts)\n",
    "    return unique_counts[y_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_star_with_smallest_count(df):\n",
    "    \"\"\"\n",
    "    Find the unique value(s) in the 'y_star' column of a DataFrame that have the smallest count.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "\n",
    "    Returns:\n",
    "    The unique value(s) in the 'y_star' column with the smallest count.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the count of each unique value in the 'y_star' column and convert it to a dictionary.\n",
    "    unique_counts = df['y_star'].value_counts().to_dict()\n",
    "    \n",
    "    # Find the smallest count value among the unique counts.\n",
    "    min_count = min(unique_counts.values())\n",
    "\n",
    "    # Construct a list of unique value(s) with the smallest count.\n",
    "    y_with_min_count = [key for key, value in unique_counts.items() if value == min_count]\n",
    "\n",
    "    # Return the first value from the list (if there are multiple values with the smallest count).\n",
    "    return y_with_min_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_filtered_for_y_value(df, y_value):\n",
    "    \"\"\"\n",
    "    Create a filtered DataFrame containing rows where the 'y' column matches a specific value.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The original DataFrame containing the data.\n",
    "    y_value: The value to filter rows based on in the 'y' column.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A new DataFrame with rows filtered for the specified y_value in the 'y' column.\n",
    "    \"\"\"\n",
    "\n",
    "    return df[df['y'] == y_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_x_count_y_to_x(df):\n",
    "    \"\"\"\n",
    "    Create a DataFrame that shows the count of occurrences of each 'x' value in the original DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The original DataFrame containing the data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A new DataFrame with 'x' values and their corresponding counts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the count of occurrences of each 'x' value and reset the index.\n",
    "    df_x_count_y_to_x = df['x'].value_counts().reset_index()\n",
    "\n",
    "    # Rename columns to 'x' and 'count_y_to_x' for clarity.\n",
    "    df_x_count_y_to_x.columns = ['x', 'count_y_to_x']\n",
    "\n",
    "    return df_x_count_y_to_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train_es' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[39m# Return the list of unique 'y' values.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m unique_values\n\u001b[1;32m---> 18\u001b[0m \u001b[39mprint\u001b[39m(create_ls_of_all_y_values(df_train_es))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train_es' is not defined"
     ]
    }
   ],
   "source": [
    "def create_ls_of_all_y_values(df):\n",
    "    \"\"\"\n",
    "    Create a list of all unique values in the 'y' column of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing all unique values in the 'y' column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the unique values from the 'y' column.\n",
    "    unique_values = df['y'].unique()\n",
    "\n",
    "    # Return the list of unique 'y' values.\n",
    "    return unique_values\n",
    "    \n",
    "print(create_ls_of_all_y_values(df_train_es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\adieu\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/adieu/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def create_df_e_x_y_train(train_df, y_value):\n",
    "    \"\"\"\n",
    "    Create a DataFrame containing conditional probabilities and 'y' values for specific conditions.\n",
    "\n",
    "    Parameters:\n",
    "    train_df (DataFrame): The training DataFrame containing the data.\n",
    "    y_value: The specific 'y' value for which conditional probabilities are calculated.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A new DataFrame with conditional probabilities and 'y' values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter the training DataFrame for the specified y_value.\n",
    "    df_train_filtered_for_y = create_df_filtered_for_y_value(train_df, y_value)\n",
    "\n",
    "    # Create a DataFrame showing the count of occurrences of each 'x' value for the specified y_value.\n",
    "    df_e_x_y_train = create_df_x_count_y_to_x(df_train_filtered_for_y)\n",
    "\n",
    "    # Conditional probability formula - P(x|y) = P(x âˆ© y) / P(y)\n",
    "    # Calculate conditional probabilities 'e(x|y)' by dividing\n",
    "    # the count of occurrences of each 'x' value for the specific 'y' value.\n",
    "    # by the count of occurrences of the specific y_value\n",
    "    df_e_x_y_train['e(x|y)'] = df_e_x_y_train['count_y_to_x'] / (count_y(train_df, y_value))\n",
    "\n",
    "    # Add 'y' column to the DataFrame with the specified y_value.\n",
    "    df_e_x_y_train['y'] = y_value\n",
    "    \n",
    "    return df_e_x_y_train\n",
    "\n",
    "# Create a DataFrame containing conditional probabilities for a specific 'y' value.\n",
    "df_e_x_y_train_for_I_neutral = create_df_e_x_y_train(df_train_es, \"B-positive\")\n",
    "\n",
    "# Print the DataFrame showing conditional probabilities.\n",
    "print(df_e_x_y_train_for_I_neutral)\n",
    "\n",
    "# Calculate and print the sum of conditional probabilities from the DataFrame. \n",
    "# Should add to 1 according to the characteristics of a probability distribution.\n",
    "print(df_e_x_y_train_for_I_neutral['e(x|y)'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\adieu\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/adieu/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def create_df_e_x_y_test(test_df, y_value, train_df):\n",
    "    \"\"\"\n",
    "    Create a DataFrame containing conditional probabilities 'e(x|y)' for a specific 'y' value using test and training data.\n",
    "\n",
    "    Parameters:\n",
    "    test_df (DataFrame): The test DataFrame containing the data to be evaluated.\n",
    "    y_value: The specific 'y' value for which conditional probabilities are calculated.\n",
    "    train_df (DataFrame): The training DataFrame containing the data used for reference.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A new DataFrame with 'x' values, calculated conditional probabilities 'e(x|y)', and a sum of probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initializing k = 1\n",
    "    k = 1\n",
    "\n",
    "    # Filtering the test DataFrame for the specified 'y' value\n",
    "    df_test_filtered_for_y = create_df_filtered_for_y_value(test_df, y_value)\n",
    "\n",
    "    # Creating a DataFrame with count of occurrences of each 'x' value for the specified 'y' value\n",
    "    # for the filtered test DataFrame\n",
    "    df_e_x_y_test = create_df_x_count_y_to_x(df_test_filtered_for_y)\n",
    "\n",
    "    # Calculating the total count of 'y' values plus a constant k (1)\n",
    "    count_y_plus_k = count_y(test_df, y_value) + k\n",
    "\n",
    "    # Extracting the 'x' values from the training DataFrame\n",
    "    train_df_x_values = train_df['x'].tolist()\n",
    "\n",
    "    # Handling unknown 'x' values by replacing them with \"#UNK#\" if not present in the training set\n",
    "    df_e_x_y_test['x'] = df_e_x_y_test['x'].apply(lambda x: x if x in train_df_x_values else \"#UNK#\")\n",
    "\n",
    "    # Calculating conditional probabilities 'e(x|y)' based on the 'x' values\n",
    "    df_e_x_y_test['e(x|y)'] = df_e_x_y_test.apply(lambda row: (row['count_y_to_x'] / count_y_plus_k) if row['x'] != '#UNK#' else (k / count_y_plus_k), axis=1)\n",
    "\n",
    "    # Printing the sum of conditional probabilities\n",
    "    # Should add to 1 according to the characteristics of the probability distribution\n",
    "    print(df_e_x_y_test['e(x|y)'].sum())\n",
    "\n",
    "    # Returning the DataFrame containing calculated conditional probabilities\n",
    "    return df_e_x_y_test\n",
    "\n",
    "# Define the file path for the test data\n",
    "file_path_test_es = 'Data/ES/dev.out'\n",
    "\n",
    "# Load the test data from the specified file path into a DataFrame\n",
    "df_test_es = file_to_df(file_path_test_es)\n",
    "\n",
    "# Create a DataFrame containing conditional probabilities for a specific 'y' value using test and training data\n",
    "df_e_x_y_test_for_I_neutral = create_df_e_x_y_test(df_test_es, \"B-positive\", df_train_es)\n",
    "\n",
    "# Print the DataFrame showing calculated conditional probabilities\n",
    "print(df_e_x_y_test_for_I_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\adieu\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/adieu/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def create_e_x_y_df_train_all_y_values(file_path):\n",
    "    \"\"\"\n",
    "    Create a combined DataFrame of conditional probabilities 'e(x|y)' for all unique 'y' values in the provided dataset.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The file path to the dataset containing 'x' and 'y' values.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing calculated conditional probabilities for each 'x' value and unique 'y' value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the data from the specified file path into a DataFrame\n",
    "    df_train = file_to_df(file_path)\n",
    "\n",
    "    # Initialize an empty list to store DataFrames containing conditional probabilities\n",
    "    ls_df_train = []\n",
    "\n",
    "    # Create a list of all unique 'y' values in the DataFrame\n",
    "    ls_y_values = create_ls_of_all_y_values(df_train)\n",
    "\n",
    "    # Iterate through each unique 'y' value\n",
    "    for y_value in ls_y_values:\n",
    "        # Skip the loop iteration if y_value is None\n",
    "        if y_value is not None:\n",
    "            # Calculate conditional probabilities for the current 'y' value and add to the list\n",
    "            df_e_x_y = create_df_e_x_y_train(df_train, y_value)\n",
    "            # Append the conditional probability for that y value to the list\n",
    "            ls_df_train.append(df_e_x_y)\n",
    "\n",
    "    # Concatenate the list of DataFrames into a single DataFrame, combining the rows (stacking the DataFrames)\n",
    "    combined_df_train = pd.concat(ls_df_train, axis=0)\n",
    "    return combined_df_train\n",
    "\n",
    "file_path_train_es = 'Data/ES/train'\n",
    "print(create_e_x_y_df_train_all_y_values(file_path_train_es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\adieu\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/adieu/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def create_df_x_to_y_star(file_path):\n",
    "\n",
    "    # Calculate conditional probabilities for all 'y' values using the training data\n",
    "    e_x_y_df_train = create_e_x_y_df_train_all_y_values(file_path)\n",
    "\n",
    "    # Group by 'x' and find the maximum 'e(x|y)' value for each group\n",
    "    df_x_to_y_star = e_x_y_df_train.groupby('x')['e(x|y)'].max().reset_index()\n",
    "\n",
    "    # Find the corresponding 'y' values for the maximum 'e(x|y)' values\n",
    "    df_x_to_y_star = pd.merge(df_x_to_y_star, e_x_y_df_train, on=['x', 'e(x|y)'])\n",
    "\n",
    "    # Rename the columns\n",
    "    df_x_to_y_star.columns = ['x', 'max_e(x|y)', 'count_y_to_x', 'y_star']\n",
    "\n",
    "    # Return the resulting DataFrame\n",
    "    return df_x_to_y_star\n",
    "\n",
    "file_path_train_es = 'Data/ES/train'\n",
    "create_df_x_to_y_star(file_path_train_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\adieu\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/adieu/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def generate_y_values(file_path_dev_in, file_path_train, file_path_dev_p1_out):\n",
    "    \"\"\"\n",
    "    Generate 'y' values using conditional probabilities and write the results to an output file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path_dev_in (str): The file path to the input dataset containing 'x' values.\n",
    "    file_path_train (str): The file path to the training dataset containing 'x' and 'y' values.\n",
    "    file_path_dev_p1_out (str): The file path to write the generated 'y' values.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate a DataFrame using the create_df_x_to_y_star function\n",
    "    df_train = create_df_x_to_y_star(file_path_train)\n",
    "\n",
    "    # Convert the x values in df_train to a list\n",
    "    x_values = df_train['x'].tolist()\n",
    "    \n",
    "    # The y_label that has the smallest count_y will generate the biggest k / count_y_plus_k,\n",
    "    # which will give the maximum e(x|y), meaning that y_label is y_star\n",
    "    y_label = y_star_with_smallest_count(df_train)\n",
    "    \n",
    "    # Open the input file for reading and use UTF-8 encoding\n",
    "    with open(file_path_dev_in, 'r', encoding='utf-8') as file:\n",
    "\n",
    "        # Read all lines from the file and store them in a list\n",
    "        lines = file.readlines()\n",
    "\n",
    "        # Iterate through each line in the list of lines\n",
    "        for l in range(len(lines)):\n",
    "\n",
    "            # Remove leading/trailing whitespace and store the current line\n",
    "            line = lines[l].strip()\n",
    "\n",
    "            # Check if the current line exists in the list of 'x_values'\n",
    "            if line in x_values:\n",
    "\n",
    "                # Get corresponding 'y_star' value(s) for the 'x' value in the current line\n",
    "                possible_y_values = df_train[df_train['x'] == line]['y_star'].tolist()\n",
    "\n",
    "                # Append the first 'y_star' value to the current line\n",
    "                lines[l] = line + \" \" + possible_y_values[0]\n",
    "\n",
    "                # Check if there are multiple 'y_star' values, indicating a potential issue\n",
    "                if len(possible_y_values) != 1:\n",
    "                    print(\"Something wrong: x_values in df_train not unique for some reason, for line:\", l)\n",
    "            \n",
    "            # If the current line is not in 'x_values'\n",
    "            else:\n",
    "                 \n",
    "                # Check if the line is not empty\n",
    "                if line != \"\\n\":\n",
    "\n",
    "                    # Append the default 'y_label' to the current line\n",
    "                    line = line + \" \" + y_label\n",
    "\n",
    "                    # Update the current line in the list\n",
    "                    lines[l] = line\n",
    "    \n",
    "    # Open the output file for writing using UTF-8 encoding\n",
    "    with open(file_path_dev_p1_out, 'w', encoding='utf-8') as file:\n",
    "        \n",
    "        # Write each modified line back to the output file\n",
    "        for line in lines:\n",
    "            if line != \"\\n\":\n",
    "                file.write(line + '\\n')\n",
    "            else:\n",
    "                file.write(line)\n",
    "\n",
    "\n",
    "file_path_dev_in_es = 'Data/ES/dev.in'\n",
    "file_path_train_es = 'Data/ES/train'\n",
    "file_path_dev_p1_out_es = 'Data/ES/dev.p1.out'\n",
    "generate_y_values(file_path_dev_in_es, file_path_train_es, file_path_dev_p1_out_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\adieu\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/adieu/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "file_path_dev_p1_out_es = 'Data/ES/dev.p1.out'\n",
    "file_dev_out_es = 'Data/ES/dev.out'\n",
    "df_dev_p1_out_es = file_to_df(file_path_dev_p1_out_es)\n",
    "df_dev_out_es = file_to_df(file_dev_out_es)\n",
    "print(df_dev_p1_out_es.shape[0])\n",
    "print(df_dev_out_es.shape[0])\n",
    "\n",
    "df1 = df_dev_p1_out_es\n",
    "df2 = df_dev_out_es\n",
    "\n",
    "# Compare rows\n",
    "comparison = df1.equals(df2)\n",
    "\n",
    "# Get the count of rows that are the same\n",
    "count_same = df1[df1.eq(df2).all(axis=1)].shape[0]\n",
    "\n",
    "# Get the count of rows that are not the same\n",
    "count_not_same = df1[df1.ne(df2).any(axis=1)].shape[0]\n",
    "\n",
    "# Get the rows that are not the same\n",
    "rows_not_same = df1[~df1.eq(df2).all(axis=1)]\n",
    "\n",
    "total_number_of_correctly_predicted_entries = count_same\n",
    "total_number_of_predicted_entities = df1.shape[0]\n",
    "precision = total_number_of_correctly_predicted_entries / total_number_of_predicted_entities\n",
    "print(precision)\n",
    "\n",
    "# Display the results\n",
    "# print(\"Comparison result:\", comparison)\n",
    "# print(\"Count of rows that are the same:\", count_same)\n",
    "# print(\"Count of rows that are not the same:\", count_not_same)\n",
    "# print(\"Rows that are not the same:\\n\", rows_not_same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ES Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\adieu\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/adieu/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Working for both ES and RU\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def load_dev_in_data(file_path):\n",
    "    \"\"\"Specific function to load dev.in data, which only contains words.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read().strip().split('\\n\\n')\n",
    "    return [[word for word in sentence.split('\\n') if word.strip()] for sentence in data]\n",
    "\n",
    "def load_data_modified_v7(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read().strip().split('\\n\\n')\n",
    "    # Filter out empty lines within sentences and handle lines with extra spaces\n",
    "    processed_data = []\n",
    "    for sentence in data:\n",
    "        processed_sentence = []\n",
    "        for line in sentence.split('\\n'):\n",
    "            if line.strip():  # Check if line is not empty\n",
    "                match = re.search(r'^(.*)\\s(\\S+)$', line)\n",
    "                if match:\n",
    "                    word, tag = match.groups()\n",
    "                    processed_sentence.append(f\"{word} {tag}\")\n",
    "        # Add the processed sentence to the data only if it's not empty\n",
    "        if processed_sentence:\n",
    "            processed_data.append(processed_sentence)\n",
    "    return processed_data\n",
    "\n",
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "    # Initialize a list to store information about the most likely states at each time step\n",
    "    V = [{}]\n",
    "    \n",
    "    # Initialization step: Calculate initial probabilities for each state based on the first observation\n",
    "    for st in states:\n",
    "        # Store the probability and mark that it has no previous state (since it's the first step)\n",
    "        V[0][st] = {\"prob\": start_p.get(st, 0) * emit_p[st].get(obs[0], 0), \"prev\": None}\n",
    "        \n",
    "        \n",
    "    # Loop through the observations and calculate the most likely sequence of states\n",
    "    for t in range(1, len(obs)):\n",
    "        # Add a new dictionary for the current time step\n",
    "        V.append({})\n",
    "        for st in states:\n",
    "            # Calculate the maximum transition probability from a previous state to the current state\n",
    "            max_trans_prob = max(V[t-1][prev_st][\"prob\"] * trans_p[prev_st].get(st, 0) for prev_st in states)\n",
    "            \n",
    "            # Find the previous state that resulted in the maximum transition probability\n",
    "            for prev_st in states:\n",
    "                if V[t-1][prev_st][\"prob\"] * trans_p[prev_st].get(st, 0) == max_trans_prob:\n",
    "                    # Calculate the maximum probability for the current state based on emission probabilities\n",
    "                    max_prob = max_trans_prob * emit_p[st].get(obs[t], 0)\n",
    "        \n",
    "                    # Store the maximum probability and the reference to the previous state\n",
    "                    V[t][st] = {\"prob\": max_prob, \"prev\": prev_st}\n",
    "                    break\n",
    "                    \n",
    "    # Backtrack to find the most likely sequence of states\n",
    "    opt = []\n",
    "    \n",
    "    # Find the maximum probability in the last time step\n",
    "    max_prob = max(value[\"prob\"] for value in V[-1].values())\n",
    "    \n",
    "    previous = None\n",
    "    # Iterate through the states in the last time step to find the state with maximum probability\n",
    "    for st, data in V[-1].items():\n",
    "        # If the probability matches the maximum probability in the last time step:\n",
    "        if data[\"prob\"] == max_prob:\n",
    "            # Add the current state to the optimal sequence\n",
    "            opt.append(st)\n",
    "            # Update the 'previous' variable with the current state\n",
    "            previous = st\n",
    "            # Stop the loop since we found the state with the maximum probability\n",
    "            break\n",
    "            \n",
    "    # Iterate backwards through time steps starting from the second-to-last\n",
    "    for t in range(len(V) - 2, -1, -1):\n",
    "        # Insert the previous state that led to the current state at the beginning of the 'opt' list\n",
    "        opt.insert(0, V[t + 1][previous][\"prev\"])\n",
    "        \n",
    "        # Update the 'previous' state with the value of the previous state from the current time step\n",
    "        previous = V[t + 1][previous][\"prev\"]\n",
    "\n",
    "    return opt\n",
    "\n",
    "def compute_probabilities_v2(data, state_list):\n",
    "    start_transition_count = {state: 0 for state in state_list}\n",
    "    transition_count = {state: {state2: 0 for state2 in state_list} for state in state_list}\n",
    "    emission_count = {state: {} for state in state_list}\n",
    "    state_count = {state: 0 for state in state_list}\n",
    "\n",
    "    for sentence in data:\n",
    "        prev_state = None\n",
    "        for line in sentence:\n",
    "            match = re.search(r'^(.*)\\s(\\S+)$', line.strip())\n",
    "            if match:\n",
    "                word, state = match.groups()\n",
    "                if prev_state is None:\n",
    "                    start_transition_count[state] += 1\n",
    "                else:\n",
    "                    transition_count[prev_state][state] += 1\n",
    "                    emission_count[prev_state][word] = emission_count[prev_state].get(word, 0) + 1\n",
    "                state_count[state] += 1\n",
    "                prev_state = state\n",
    "        if prev_state:\n",
    "            emission_count[prev_state][word] = emission_count[prev_state].get(word, 0) + 1\n",
    "\n",
    "    total_sentences = len(data)\n",
    "    start_transition_prob = {state: count / total_sentences for state, count in start_transition_count.items()}\n",
    "    transition_prob = {state: {state2: count2 / state_count[state] for state2, count2 in count.items()} for state, count in transition_count.items()}\n",
    "    emission_prob = {state: {word: count / state_count[state] for word, count in state_emission_count.items()} for state, state_emission_count in emission_count.items()}\n",
    "\n",
    "    return start_transition_prob, transition_prob, emission_prob\n",
    "\n",
    "def extract_entities_from_tags(tags):\n",
    "    entities = []\n",
    "    entity = []\n",
    "    for tag in tags:\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if entity:\n",
    "                entities.append(tuple(entity))\n",
    "                entity = []\n",
    "            entity.append(tag)\n",
    "        elif tag.startswith(\"I-\"):\n",
    "            entity.append(tag)\n",
    "        else:\n",
    "            if entity:\n",
    "                entities.append(tuple(entity))\n",
    "                entity = []\n",
    "    if entity:\n",
    "        entities.append(tuple(entity))\n",
    "    return set(entities)\n",
    "\n",
    "# Modify the process_dataset function to use the updated compute_probabilities function\n",
    "def process_dataset_final_v5(dataset_type):\n",
    "    # Adjusting the paths dynamically based on dataset type\n",
    "    train_path = f\"Data/{dataset_type}/train\"\n",
    "    dev_in_path = f\"Data/{dataset_type}/dev.in\"\n",
    "    dev_out_path = f\"Data/{dataset_type}/dev.out\"\n",
    "    \n",
    "    train_data = load_data_modified_v7(train_path)\n",
    "    dev_in_data = load_dev_in_data(dev_in_path)\n",
    "    with open(dev_out_path, 'r', encoding='utf-8') as f:\n",
    "        dev_tags_actual = [sentence.split() for sentence in f.read().strip().split('\\n\\n')]\n",
    "    \n",
    "    states = {}\n",
    "    observations = {}\n",
    "\n",
    "    for sentence in train_data:\n",
    "        for line in sentence:\n",
    "            match = re.search(r'^(.*)\\s(\\S+)$', line.strip())\n",
    "            if match:\n",
    "                word, tag = match.groups()\n",
    "                states[tag] = states.get(tag, 0) + 1\n",
    "                if tag not in observations:\n",
    "                    observations[tag] = {}\n",
    "                observations[tag][word] = observations[tag].get(word, 0) + 1\n",
    "\n",
    "    state_list = list(states.keys())\n",
    "    start_transition_prob, transition_prob, emission_prob = compute_probabilities_v2(train_data, state_list)\n",
    "    predicted_tags_viterbi = [viterbi([word for word in sentence], state_list, start_transition_prob, transition_prob, emission_prob) for sentence in dev_in_data]\n",
    "    \n",
    "#     print(predicted_tags_viterbi)\n",
    "\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "\n",
    "    for pred, actual in zip(predicted_tags_viterbi, dev_tags_actual):\n",
    "        predicted_entities = extract_entities_from_tags(pred)\n",
    "        actual_entities = extract_entities_from_tags(actual)\n",
    "        TP += len(predicted_entities.intersection(actual_entities))\n",
    "        FP += len(predicted_entities - actual_entities)\n",
    "        FN += len(actual_entities - predicted_entities)\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    f_score = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    \n",
    "#     predicted = []\n",
    "#     for s in range(len(predicted_tags_viterbi)):\n",
    "#         for i in range(len(predicted_tags_viterbi[s])):\n",
    "#             predicted.append(dev_in_data[s][i] + \" \"+ predicted_tags_viterbi[s][i])\n",
    "#         predicted.append('\\n')\n",
    "        \n",
    "#     # Open the output file for writing using UTF-8 encoding\n",
    "#     file_path_dev_p2_out = 'Data/ES/dev.p1.out'\n",
    "#     with open(file_path_dev_p2_out, 'w', encoding='utf-8') as file:\n",
    "#         # Write each modified line back to the output file\n",
    "#         for line in predicted:\n",
    "#             if line != \"\\n\":\n",
    "#                 file.write(line + '\\n')\n",
    "#             else:\n",
    "#                 file.write(line)\n",
    "\n",
    "# process_dataset_final_v5(\"ES\")\n",
    "    return precision, recall, f_score\n",
    "\n",
    "# Process the datasets using the latest modifications\n",
    "es_results_final_v10 = process_dataset_final_v5(\"ES\")\n",
    "ru_results_final_v10 = process_dataset_final_v5(\"RU\")\n",
    "\n",
    "es_results_final_v10, ru_results_final_v10\n",
    "\n",
    "# Print the precision, recall, and F-score for the \"ES\" dataset\n",
    "print(\"ES Dataset Results:\")\n",
    "print(\"Precision:\", es_results_final_v10[0])\n",
    "print(\"Recall:\", es_results_final_v10[1])\n",
    "print(\"F-score:\", es_results_final_v10[2])\n",
    "\n",
    "# Print the precision, recall, and F-score for the \"RU\" dataset\n",
    "print(\"\\nRU Dataset Results:\")\n",
    "print(\"Precision:\", ru_results_final_v10[0])\n",
    "print(\"Recall:\", ru_results_final_v10[1])\n",
    "print(\"F-score:\", ru_results_final_v10[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "\n",
    "sr_start_probabilities_es = start_transition_prob\n",
    "df_transition_es = transition_prob\n",
    "\n",
    "sr_start_probabilities_ru = start_transition_prob\n",
    "df_transition_ru = transition_prob\n",
    "\n",
    "sr_start_probabilities_es = pd.Series(sr_start_probabilities_es)\n",
    "df_transition_es = pd.DataFrame(df_transition_es).T\n",
    "df_emission_es = pd.read_csv('Data/ES/csv_dev_out_es_test_e_x_y.csv')\n",
    "df_emission_es = df_emission_es.drop_duplicates(subset=['x','y'], keep='last')\n",
    "df_emission_es = df_emission_es.pivot(index='y', columns='x', values='e(x|y)').fillna(0)\n",
    "display(df_emission_es)\n",
    "\n",
    "sr_start_probabilities_ru = pd.Series(sr_start_probabilities_ru)\n",
    "df_transition_ru = pd.DataFrame(df_transition_ru).T\n",
    "df_emission_ru = pd.read_csv('Data/RU/csv_dev_out_ru_test_e_x_y.csv')\n",
    "df_emission_ru = df_emission_ru.drop_duplicates(subset=['x','y'], keep='last')\n",
    "df_emission_ru = df_emission_ru.pivot(index='y', columns='x', values='e(x|y)').fillna(0)\n",
    "display(df_emission_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_viterbi(observation, transition, emission, start_probabilities, kth_best):\n",
    "    k_best = math.ceil(kth_best/3)\n",
    "    states = transition.index.tolist()\n",
    "    step_count = len(observation)\n",
    "\n",
    "    preceding = {}\n",
    "\n",
    "    preceding = {state: [(0, [])] * k_best for state in states}\n",
    "\n",
    "    for state, sequences in preceding.items():\n",
    "        \n",
    "        try:\n",
    "            preceding[state][0] = (emission[observation[0]][state] * start_probabilities[state], [state])\n",
    "        except:\n",
    "            preceding[state][0] = (emission[\"#UNK#\"][state] * start_probabilities[state], [state])\n",
    "\n",
    "    for step in range(1, step_count):\n",
    "\n",
    "        # refresh current at the beginning of each step\n",
    "        current = {state: [(0, [])] * k_best for state in states}\n",
    "\n",
    "        for current_state in states:\n",
    "\n",
    "            for previous_state in states:\n",
    "\n",
    "                for sequence in preceding[previous_state]:\n",
    "\n",
    "                    # getting the transition probability from preceding state to current state from one of the tuples in the \"preceding\" table\n",
    "                    prev_probability = sequence[0]\n",
    "\n",
    "                    try:\n",
    "                        emission_param = emission.loc[current_state, observation[step]]\n",
    "                    except:\n",
    "                        emission_param = emission.loc[current_state, \"#UNK#\"]\n",
    "                    prev_to_cur_probability = prev_probability * transition.loc[previous_state, current_state] * emission_param\n",
    "\n",
    "                    # sort the tuples in ascending order so that the first tuple has lowest probability\n",
    "                    current[current_state] = sorted(current[current_state], key=lambda x: x[0])\n",
    "                    \n",
    "                    if prev_to_cur_probability >= current[current_state][0][0]:\n",
    "                        sequence_list = copy.deepcopy(sequence[1])\n",
    "                        sequence_list.append(current_state)\n",
    "                        current[current_state][0] = (prev_to_cur_probability, sequence_list)\n",
    "        \n",
    "        # we are either entering the next step or leaving the loop, so preceding becomes current\n",
    "        preceding = copy.deepcopy(current)\n",
    "\n",
    "    combined_list = []\n",
    "    for sequences in preceding.values():\n",
    "        combined_list.extend(sequences)\n",
    "\n",
    "    combined_list = sorted(combined_list, key=lambda x: x[0])[::-1]\n",
    "\n",
    "    return combined_list[kth_best-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/ES/dev.in'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m         data \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m [sentence\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m data]\n\u001b[1;32m----> 6\u001b[0m es_observations \u001b[39m=\u001b[39m load_data(\u001b[39m'\u001b[39;49m\u001b[39mData/ES/dev.in\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m ru_observations \u001b[39m=\u001b[39m load_data(\u001b[39m'\u001b[39m\u001b[39mData/RU/dev.in\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m predicted_tags_viterbi_ES2 \u001b[39m=\u001b[39m [modified_viterbi([word\u001b[39m.\u001b[39msplit()[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m sentence], df_transition_es, df_emission_es, sr_start_probabilities_es, \u001b[39m2\u001b[39m)[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m es_observations]\n",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_data\u001b[39m(file_path):\n\u001b[1;32m----> 2\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m      3\u001b[0m         data \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m [sentence\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m data]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/ES/dev.in'"
     ]
    }
   ],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read().strip().split('\\n\\n')\n",
    "    return [sentence.split('\\n') for sentence in data]\n",
    "\n",
    "es_observations = load_data('Data/ES/dev.in')\n",
    "ru_observations = load_data('Data/RU/dev.in')\n",
    "\n",
    "predicted_tags_viterbi_ES2 = [modified_viterbi([word.split()[0] for word in sentence], df_transition_es, df_emission_es, sr_start_probabilities_es, 2)[1] for sentence in es_observations]\n",
    "predicted_tags_viterbi_ES8 = [modified_viterbi([word.split()[0] for word in sentence], df_transition_es, df_emission_es, sr_start_probabilities_es, 8)[1] for sentence in es_observations]\n",
    "predicted_tags_viterbi_RU2 = [modified_viterbi([word.split()[0] for word in sentence], df_transition_ru, df_emission_ru, sr_start_probabilities_ru, 2)[1] for sentence in ru_observations]\n",
    "predicted_tags_viterbi_RU8 = [modified_viterbi([word.split()[0] for word in sentence], df_transition_ru, df_emission_ru, sr_start_probabilities_ru, 8)[1] for sentence in ru_observations]\n",
    "\n",
    "predicted_ES2 = []\n",
    "for s in range(len(predicted_tags_viterbi_ES2)):\n",
    "    for i in range(len(predicted_tags_viterbi_ES2[s])):\n",
    "        predicted_ES2.append(es_observations[s][i] + \" \"+ predicted_tags_viterbi_ES2[s][i])\n",
    "    predicted_ES2.append('\\n')\n",
    "\n",
    "predicted_ES8 = []\n",
    "for s in range(len(predicted_tags_viterbi_ES8)):\n",
    "    for i in range(len(predicted_tags_viterbi_ES8[s])):\n",
    "        predicted_ES8.append(es_observations[s][i] + \" \"+ predicted_tags_viterbi_ES8[s][i])\n",
    "    predicted_ES2.append('\\n')\n",
    "\n",
    "predicted_RU2 = []\n",
    "for s in range(len(predicted_tags_viterbi_RU2)):\n",
    "    for i in range(len(predicted_tags_viterbi_RU2[s])):\n",
    "        predicted_RU2.append(ru_observations[s][i] + \" \"+ predicted_tags_viterbi_RU2[s][i])\n",
    "    predicted_RU2.append('\\n')\n",
    "\n",
    "predicted_RU8 = []\n",
    "for s in range(len(predicted_tags_viterbi_RU8)):\n",
    "    for i in range(len(predicted_tags_viterbi_RU8[s])):\n",
    "        predicted_RU8.append(ru_observations[s][i] + \" \"+ predicted_tags_viterbi_RU8[s][i])\n",
    "    predicted_ES8.append('\\n')\n",
    "\n",
    "with open('Data/ES/dev.p3.2nd.out', 'w', encoding='utf-8') as file:\n",
    "    # Write each modified line back to the output file\n",
    "    for line in predicted_ES2:\n",
    "        if line != \"\\n\":\n",
    "            file.write(line + '\\n')\n",
    "        else:\n",
    "            file.write(line)\n",
    "\n",
    "with open('Data/ES/dev.p3.8th.out', 'w', encoding='utf-8') as file:\n",
    "    # Write each modified line back to the output file\n",
    "    for line in predicted_ES8:\n",
    "        if line != \"\\n\":\n",
    "            file.write(line + '\\n')\n",
    "        else:\n",
    "            file.write(line)\n",
    "\n",
    "with open('Data/RU/dev.p3.2nd.out', 'w', encoding='utf-8') as file:\n",
    "    # Write each modified line back to the output file\n",
    "    for line in predicted_RU2:\n",
    "        if line != \"\\n\":\n",
    "            file.write(line + '\\n')\n",
    "        else:\n",
    "            file.write(line)\n",
    "\n",
    "with open('Data/RU/dev.p3.8th.out', 'w', encoding='utf-8') as file:\n",
    "    # Write each modified line back to the output file\n",
    "    for line in predicted_RU8:\n",
    "        if line != \"\\n\":\n",
    "            file.write(line + '\\n')\n",
    "        else:\n",
    "            file.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
